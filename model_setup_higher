# Model setup for 3 and 6 dimensions (first 6D, then 3D)

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F


# 6D:
# Define an RNN with outputs representing the parameters of a Gaussian mixture density model.
# The first n output parameters are pi_i, the next 6n parameters are mu_i, and the last 21n parameters are Sigma_i.
# The loss function: negative log-likelihood function.

class GaussianMixtureModel():
    def __init__(self, params):
        self.pi, self.mu, self.Sigma = params
        self.pi = self.pi.reshape(-1).numpy()
        self.mu = self.mu.reshape(-1, 6).numpy()
        self.Sigma = self.Sigma.reshape(-1, 6, 6).numpy()

        #self.mu = self.mu.reshape(-1, 2).numpy()
        #self.Sigma = self.Sigma.reshape(-1, 2, 2).numpy()

    def sample(self, num_samples):
        # Randomly draw num_samples samples
        # Select the i-th Gaussian distribution according to the probability pi_i, then sample num_samples samples from that Gaussian distribution
        samples = []
        for _ in range(num_samples):
            idx = np.random.choice(len(self.pi), p=self.pi)
            samples.append(np.random.multivariate_normal(self.mu[idx], self.Sigma[idx]))
        return np.array(samples)

    def pdf(self, x):
        # The probability density value of x
        prob = 0
        for i in range(len(self.pi)):
          diff = x - self.mu[i]
          prob += self.pi[i] * np.exp(-0.5 * diff.T @ np.linalg.inv(self.Sigma[i]) @ diff) / \
                   ((2*np.pi)**3 * np.sqrt(np.linalg.det(self.Sigma[i]) + 1e-8))

        return prob

    def __str__(self):
        str = ''
        for i in range(len(self.pi)):
            str += 'pi_{}: {}\n'.format(i, self.pi[i])
            str +='mu_{}: {}\n'.format(i, self.mu[i])
            str += 'Sigma_{}:\n{}\n\n'.format(i, self.Sigma[i])
        return str

class RNN(nn.Module):
    #def __init__(self, gause_mixture_n = 10, input_size = 2, hidden_size = 30):
    def __init__(self, gause_mixture_n = 10, input_size = 6, hidden_size = 30):
        super(RNN, self).__init__()
        self.RNN = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc1 = nn.Linear(hidden_size, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, gause_mixture_n * (1 + 6 + 21))

        #self.fc3 = nn.Linear(64, gause_mixture_n * 6)  # n + 2n + 3n = 6n

        self.relu = nn.ReLU()
        self.softmax = nn.Softmax(dim=1)
        self.softplus = nn.Softplus()
        self.n = gause_mixture_n
        self.dim = 6

    def forward(self, x):
        out, _ = self.RNN(x)
        out = self.fc1(out)
        out = self.relu(out)
        out = self.fc2(out)
        out = self.relu(out)
        out = self.fc3(out)
        out = out[:, -1]
        # shape of y_pred: (batch_size, 70) -> (batch_size, 70)
    #    print('out: ', out.shape)

        pi_pred, mu_pred, Sigma_pred = torch.split(out, [self.n, self.n*self.dim, self.n*21], dim=1)

        pi_pred = self.softmax(pi_pred)
        pi_pred = pi_pred.reshape(-1, self.n, 1)

        mu_pred = mu_pred.reshape(-1, self.n, 3).reshape(-1, self.n, 6, 1)

        # covariance matrix
        Sigma_pred = Sigma_pred.reshape(-1, self.n, 21)

        # Cholesky decomposition
        L = torch.zeros(x.size(0), self.n, self.dim, self.dim, device=x.device)
        tril_indices = torch.tril_indices(self.dim, self.dim)
        L[:, :, tril_indices[0], tril_indices[1]] = Sigma_pred

        diag_indices = torch.arange(self.dim)
        L[:, :, diag_indices, diag_indices] = self.softplus(L[:, :, diag_indices, diag_indices]) + 1e-2

        Sigma_pred = torch.matmul(L, L.transpose(2, 3))

        is_pos_def = torch.all(torch.det(Sigma_pred) >= -1e-8)
        if not is_pos_def:
            print('Sigma_pred is not positive definite')
            print(torch.det(Sigma_pred))
            print(torch.min(torch.det(Sigma_pred)))
            exit(0)

        return (pi_pred, mu_pred, Sigma_pred)


    def loss_func(self, output, x_true):

        pi_pred, mu_pred, Sigma_pred = output
        # shape of x_true: (batch_size, 6) -> (batch_size, 6, 1)
        #x_true = x_true.reshape(-1, 2, 1)
        x_true = x_true.reshape(-1, 6, 1)

        # Compute the negative log-likelihood function for each sample
        prob = torch.zeros(x_true.shape[0])
        for i in range(self.n):
          diff = x_true - mu_pred[:, i]

          # 6D exponent term
          exponent = -0.5 * torch.matmul(
              torch.matmul(diff.transpose(1, 2), torch.inverse(Sigma_pred[:, i])),
              diff
          ).reshape(-1)
          # 6D constant term (2Ï€)^{6/2}
          denominator = (2 * np.pi) ** (self.dim / 2) * torch.sqrt(torch.det(Sigma_pred[:, i]) + 1e-8)
          prob += pi_pred[:, i, 0] * torch.exp(exponent) / denominator

        # Check whether 'prob' contains NaN, infinity, or zero
        prob += 1e-6
        if torch.isnan(prob).any() or torch.isinf(prob).any() or (prob == 0).any():
            print('prob has nan or inf or 0')
            print('prob:', prob)
            print('Sigma_det:', torch.det(Sigma_pred))
            print('minsigma_pred:', torch.min(torch.det(Sigma_pred)))
            exit(0)

        loss = -torch.log(prob)
    #    print('loss:', loss)
    #    print('loss mean:', loss.mean())
        return loss.mean()

    def predict_model(self, x):
        # Predictive Model
        #x = torch.tensor(x, dtype=torch.float64).reshape(1, -1, 2)
        x = torch.tensor(x, dtype=torch.float64).reshape(1, -1, 6)
        with torch.no_grad():
            output = self(x)
        return GaussianMixtureModel(output)


def train(model, x_train, y_train, x_val, y_val, lr=3e-4, batch_size=128, epochs=100):
    optimizer = optim.Adam(model.parameters(), lr=lr)
    # Divide x_train into small batches of size batch_size and convert them to tensors (float32)
    x_train = torch.tensor(x_train, dtype=torch.float64)
    y_train = torch.tensor(y_train, dtype=torch.float64)
    x_val = torch.tensor(x_val, dtype=torch.float64)
    y_val = torch.tensor(y_val, dtype=torch.float64)
    # Manually divide the dataset into small batches of batch_size
    train_loader = torch.utils.data.DataLoader(
        torch.utils.data.TensorDataset(x_train, y_train),
        batch_size=batch_size, shuffle=True)

    train_loss_list = []
    val_loss_list = []

    for epoch in range(epochs):
        total_loss = 0
        for x_batch, y_batch in train_loader:
            optimizer.zero_grad()
            output = model(x_batch)
            loss = model.loss_func(output, y_batch)
            loss.backward()
            optimizer.step()
            total_loss += loss.item() * len(x_batch)
        total_loss /= len(x_train)
        val_loss = 0
        # Validate the model 
        with torch.no_grad():
            y_pred = model(x_val)
            val_loss = model.loss_func(y_pred, y_val)
        print('Epoch:', epoch, 'Loss:', total_loss, 'Val Loss:', val_loss.item())
        train_loss_list.append(total_loss)
        val_loss_list.append(val_loss.item())
    return model, train_loss_list, val_loss_list







