# Model setup 

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

# Define an RNN with outputs representing the parameters of a Gaussian mixture density model.
# The first n output parameters are pi_i, the next 2n parameters are mu_i, and the last 3n parameters are Sigma_i.
# The loss function: negative log-likelihood function.

class GaussianMixtureModel():
    def __init__(self, params):
        self.pi, self.mu, self.Sigma = params
        self.pi = self.pi.reshape(-1).numpy()
        self.mu = self.mu.reshape(-1, 2).numpy()
        self.Sigma = self.Sigma.reshape(-1, 2, 2).numpy()

    def sample(self, num_samples):
        # Randomly draw num_samples samples
        # Select the i-th Gaussian distribution according to the probability pi_i, then sample num_samples samples from that Gaussian distribution
        samples = []
        for _ in range(num_samples):
            idx = np.random.choice(len(self.pi), p=self.pi)
            samples.append(np.random.multivariate_normal(self.mu[idx], self.Sigma[idx]))
        return np.array(samples)

    def pdf(self, x):
        # The probability density value of x
        prob = 0
        for i in range(len(self.pi)):
            prob += self.pi[i] * np.exp(-0.5 * np.matmul( np.matmul((x - self.mu[i]).transpose(),
                                                                    np.linalg.inv(self.Sigma[i])),
                                                                     (x - self.mu[i]))) / (2*np.pi*np.sqrt(np.linalg.det(self.Sigma[i]) + 1e-8))
        return prob

    def __str__(self):
        str = ''
        for i in range(len(self.pi)):
            str += 'pi_{}: {}\n'.format(i, self.pi[i])
            str +='mu_{}: {}\n'.format(i, self.mu[i])
            str += 'Sigma_{}:\n{}\n\n'.format(i, self.Sigma[i])
        return str

class RNN(nn.Module):
    def __init__(self, gause_mixture_n = 10, input_size = 2, hidden_size = 30):
        super(RNN, self).__init__()
        self.RNN = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc1 = nn.Linear(hidden_size, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, gause_mixture_n * 6)
        self.relu = nn.ReLU()
        self.softmax = nn.Softmax(dim=1)
        self.softplus = nn.Softplus()
        self.n = gause_mixture_n

    def forward(self, x):
        out, _ = self.RNN(x)
        out = self.fc1(out)
        out = self.relu(out)
        out = self.fc2(out)
        out = self.relu(out)
        out = self.fc3(out)
        out = out[:, -1]
        # shape of y_pred: (batch_size, 70) -> (batch_size, 70)
    #    print('out: ', out.shape)
        pi_pred, mu_pred, Sigma_pred = torch.split(out, [self.n, self.n*2, self.n*3], dim=1)
        # shape of pi_pred: (batch_size, 10) -> (batch_size, 10, 1)
        # shape of mu_pred: (batch_size, 20) -> (batch_size, 10, 2, 1)
        # shape of Sigma_pred: (batch_size, 30) -> (batch_size, 10, 2, 2)
        pi_pred = self.softmax(pi_pred)
        pi_pred = pi_pred.reshape(-1, self.n, 1)

        mu_pred = mu_pred.reshape(-1, self.n, 2).reshape(-1, self.n, 2, 1)


        Sigma_pred = Sigma_pred.reshape(-1, self.n, 3)
        a = Sigma_pred[:, :, 0].reshape(-1, self.n, 1, 1)
        b = Sigma_pred[:, :, 1].reshape(-1, self.n, 1, 1)
        c = Sigma_pred[:, :, 2].reshape(-1, self.n, 1, 1)
        zero = torch.zeros_like(a)
        a = self.softplus(a) + 1e-2
        c = self.softplus(c) + 1e-2
        Sigma_pred = torch.cat((torch.cat((a, b), dim=3),  # Concatenate a and b
                                torch.cat((zero, c), dim=3)),  # Concatenate 0 and c
                                dim=2)
        Sigma_pred = torch.matmul(Sigma_pred, Sigma_pred.transpose(2, 3))

        is_pos_def = torch.all(torch.det(Sigma_pred) >= -1e-8)
        if not is_pos_def:
            print('Sigma_pred is not positive definite')
            print(torch.det(Sigma_pred))
            print(torch.min(torch.det(Sigma_pred)))
            exit(0)

        return (pi_pred, mu_pred, Sigma_pred)

    def loss_func(self, output, x_true):

        pi_pred, mu_pred, Sigma_pred = output
        # shape of x_true: (batch_size, 2) -> (batch_size, 2, 1)
        x_true = x_true.reshape(-1, 2, 1)
        # Compute the negative log-likelihood function for each sample
        prob = torch.zeros(x_true.shape[0])
        for i in range(self.n):
            prob += pi_pred[:, i, 0] * torch.exp(-0.5 *
                torch.matmul(torch.matmul((x_true - mu_pred[:, i]).transpose(1, 2),
                                            torch.inverse(Sigma_pred[:, i])),
                                            x_true - mu_pred[:, i]).reshape(-1))/(torch.sqrt(torch.det(Sigma_pred[:, i])+1e-8)*(2*np.pi))
        # Check whether 'prob' contains NaN, infinity, or zero
        prob += 1e-6
        if torch.isnan(prob).any() or torch.isinf(prob).any() or (prob == 0).any():
            print('prob has nan or inf or 0')
            print('prob:', prob)
            print('Sigma_det:', torch.det(Sigma_pred))
            print('minsigma_pred:', torch.min(torch.det(Sigma_pred)))
            exit(0)


        loss = -torch.log(prob)
    #    print('loss:', loss)
    #    print('loss mean:', loss.mean())
        return loss.mean()

    def predict_model(self, x):
        # Predictive Model
        x = torch.tensor(x, dtype=torch.float64).reshape(1, -1, 2)
        with torch.no_grad():
            output = self(x)
        return GaussianMixtureModel(output)


def train(model, x_train, y_train, x_val, y_val, lr=3e-4, batch_size=128, epochs=100):
    optimizer = optim.Adam(model.parameters(), lr=lr)
    # Divide x_train into small batches of size batch_size and convert them to tensors (float32)
    x_train = torch.tensor(x_train, dtype=torch.float64)
    y_train = torch.tensor(y_train, dtype=torch.float64)
    x_val = torch.tensor(x_val, dtype=torch.float64)
    y_val = torch.tensor(y_val, dtype=torch.float64)
    # Manually divide the dataset into small batches of batch_size
    train_loader = torch.utils.data.DataLoader(
        torch.utils.data.TensorDataset(x_train, y_train),
        batch_size=batch_size, shuffle=True)

    train_loss_list = []
    val_loss_list = []

    for epoch in range(epochs):
        total_loss = 0
        for x_batch, y_batch in train_loader:
            optimizer.zero_grad()
            output = model(x_batch)
            loss = model.loss_func(output, y_batch)
            loss.backward()
            optimizer.step()
            total_loss += loss.item() * len(x_batch)
        total_loss /= len(x_train)
        val_loss = 0
        # Validate the model 
        with torch.no_grad():
            y_pred = model(x_val)
            val_loss = model.loss_func(y_pred, y_val)
        print('Epoch:', epoch, 'Loss:', total_loss, 'Val Loss:', val_loss.item())
        train_loss_list.append(total_loss)
        val_loss_list.append(val_loss.item())
    return model, train_loss_list, val_loss_list








